{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9490cc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import librosa\n",
    "import torchaudio\n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "from torchaudio.transforms import MelSpectrogram, AmplitudeToDB\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f66269e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68c98a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 3080\n",
      "Available GPU memory: 10.38 GB\n"
     ]
    }
   ],
   "source": [
    "# Check GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"Available GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"No GPUs found. Running on CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cebe3610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff88cc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DATA_PATH = '/home/akshay/project_EE708/Music Genre Classification/Train'\n",
    "SAMPLE_RATE = 22050\n",
    "DURATION = 30  # seconds\n",
    "SAMPLES_PER_TRACK = SAMPLE_RATE * DURATION\n",
    "NUM_MFCC = 13\n",
    "N_FFT = 2048\n",
    "HOP_LENGTH = 512\n",
    "NUM_SEGMENTS = 5\n",
    "SEGMENT_DURATION = 6\n",
    "SEGMENT_SAMPLES = SAMPLE_RATE * SEGMENT_DURATION\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 50  \n",
    "IMG_SIZE = (256, 256)\n",
    "LEARNING_RATE = 0.0007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb24188f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Mapping class names to indices\n",
    "CLASSES = [\n",
    "    'blues', 'classical', 'country', 'disco', 'hiphop',\n",
    "    'jazz', 'metal', 'pop', 'reggae', 'rock'\n",
    "]\n",
    "CLASS_MAPPING = {class_name: i for i, class_name in enumerate(CLASSES)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8f6b68",
   "metadata": {},
   "source": [
    "class MusicGenreDataset(Dataset):\n",
    "    \"\"\"Dataset for music genre classification with on-the-fly feature extraction\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c63c9a",
   "metadata": {},
   "source": [
    "    def __init__(self, file_paths, labels, segment_indices):\n",
    "        \"\"\"\n",
    "        Initialize the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9008bdb0",
   "metadata": {},
   "source": [
    "        Args:\n",
    "            file_paths (list): List of audio file paths\n",
    "            labels (list): List of labels corresponding to the files\n",
    "            segment_indices (list): List of segment indices to extract from each file\n",
    "        \"\"\"\n",
    "        self.file_paths = file_paths\n",
    "        self.file_paths.remove('/home/akshay/project_EE708/Music Genre Classification/Train/jazz/jazz.00054.wav')\n",
    "        self.labels = labels\n",
    "        self.segment_indices = segment_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5d9ee7",
   "metadata": {},
   "source": [
    "    def __len__(self):\n",
    "        return len(self.file_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bce19d",
   "metadata": {},
   "source": [
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Load and process an audio file on-the-fly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00163e8e",
   "metadata": {},
   "source": [
    "        Args:\n",
    "            idx: Index of the item to load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bf2ad1",
   "metadata": {},
   "source": [
    "        Returns:\n",
    "            tuple: (feature, label) where feature is the processed audio and\n",
    "                  label is the class index\n",
    "        \"\"\"\n",
    "        file_path = self.file_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        segment_idx = self.segment_indices[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5e80b7",
   "metadata": {},
   "source": [
    "        # Extract features from the audio file\n",
    "        feature = self.extract_features(file_path, segment_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac65797b",
   "metadata": {},
   "source": [
    "        # Convert to PyTorch tensors\n",
    "        feature_tensor = torch.from_numpy(feature).float()\n",
    "        label_tensor = torch.tensor(label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e6fef4",
   "metadata": {},
   "source": [
    "        # Add channel dimension for CNN\n",
    "        if feature_tensor.dim() == 2:\n",
    "            feature_tensor = feature_tensor.unsqueeze(0)  # Add channel dimension (C, H, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69212ba",
   "metadata": {},
   "source": [
    "        return feature_tensor, label_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527563d6",
   "metadata": {},
   "source": [
    "    def extract_features(self, file_path, segment_idx=0):\n",
    "        \"\"\"\n",
    "        Extract features from a specific segment of an audio file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f602e4a",
   "metadata": {},
   "source": [
    "        Args:\n",
    "            file_path: Path to the audio file\n",
    "            segment_idx: Index of the segment to extract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db9ccfd",
   "metadata": {},
   "source": [
    "        Returns:\n",
    "            numpy.ndarray: Extracted features\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Load audio file\n",
    "            y, sr = librosa.load(file_path, sr=SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975d55d0",
   "metadata": {},
   "source": [
    "            # Ensure consistent length\n",
    "            if len(y) > SAMPLES_PER_TRACK:\n",
    "                y = y[:SAMPLES_PER_TRACK]\n",
    "            else:\n",
    "                y = np.pad(y, (0, max(0, SAMPLES_PER_TRACK - len(y))), 'constant')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2d2ff8",
   "metadata": {},
   "source": [
    "            # Extract the specific segment\n",
    "            start_sample = segment_idx * SEGMENT_SAMPLES\n",
    "            end_sample = start_sample + SEGMENT_SAMPLES\n",
    "            segment = y[start_sample:end_sample]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aec2c25",
   "metadata": {},
   "source": [
    "            # Extract mel spectrogram\n",
    "            mel_spec = librosa.feature.melspectrogram(\n",
    "                y=segment,\n",
    "                sr=SAMPLE_RATE,\n",
    "                n_fft=N_FFT,\n",
    "                hop_length=HOP_LENGTH,\n",
    "                n_mels=128,\n",
    "                fmin=20,\n",
    "                fmax=8000\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6431661",
   "metadata": {},
   "source": [
    "            # Convert to log scale (dB)\n",
    "            log_mel_spec = librosa.power_to_db(mel_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dc6b90",
   "metadata": {},
   "source": [
    "            # Resize to target dimensions for the CNN\n",
    "            log_mel_spec = np.resize(log_mel_spec, IMG_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ca7732",
   "metadata": {},
   "source": [
    "            # Normalize features\n",
    "            log_mel_spec = (log_mel_spec - np.mean(log_mel_spec)) / (np.std(log_mel_spec) + 1e-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bf631e",
   "metadata": {},
   "source": [
    "            return log_mel_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38c4044",
   "metadata": {},
   "source": [
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path} (segment {segment_idx}): {str(e)}\")\n",
    "            # Return a zero array as fallback\n",
    "            return np.zeros(IMG_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db5ddb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OnTheFlyMusicGenreDataset(Dataset):\n",
    "    def __init__(self, file_list, segment_indices, transform=None, mode='train'):\n",
    "        self.file_list = file_list\n",
    "        if '/home/akshay/project_EE708/Music Genre Classification/Train/jazz/jazz.00054.wav' in self.file_list:\n",
    "            self.file_list.remove('/home/akshay/project_EE708/Music Genre Classification/Train/jazz/jazz.00054.wav')\n",
    "        self.segment_indices = segment_indices\n",
    "        self.transform = transform\n",
    "        self.mode = mode\n",
    "        \n",
    "        # Audio transforms\n",
    "        self.mel_spectrogram = MelSpectrogram(\n",
    "            sample_rate=SAMPLE_RATE,\n",
    "            n_fft=N_FFT,\n",
    "            hop_length=HOP_LENGTH,\n",
    "            n_mels=128,\n",
    "            f_min=20,\n",
    "            f_max=8000\n",
    "        )\n",
    "        self.amplitude_to_db = AmplitudeToDB()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.segment_indices)\n",
    "    \n",
    "    def _load_audio(self, file_path):\n",
    "        \"\"\"Load and preprocess audio file\"\"\"\n",
    "        # print('loading')\n",
    "        try:\n",
    "            waveform, sr = torchaudio.load(file_path)\n",
    "        except:\n",
    "            print('----------')\n",
    "            print(file_path)\n",
    "            print('----------')\n",
    "            sr = 44100\n",
    "            return torch.ones((1,int(sr*10)))\n",
    "        # print('loaded')\n",
    "        # Resample if necessary\n",
    "        if sr != SAMPLE_RATE:\n",
    "            resampler = torchaudio.transforms.Resample(sr, SAMPLE_RATE)\n",
    "            waveform = resampler(waveform)\n",
    "            \n",
    "        # Ensure correct length\n",
    "        if waveform.shape[1] < SAMPLES_PER_TRACK:\n",
    "            padding = SAMPLES_PER_TRACK - waveform.shape[1]\n",
    "            waveform = F.pad(waveform, (0, padding))\n",
    "        else:\n",
    "            waveform = waveform[:, :SAMPLES_PER_TRACK]\n",
    "            \n",
    "        return waveform.squeeze()\n",
    "\n",
    "    def _extract_features(self, waveform):\n",
    "        \"\"\"Extract mel spectrogram features\"\"\"\n",
    "        # Generate mel spectrogram\n",
    "        mel_spec = self.mel_spectrogram(waveform)\n",
    "        \n",
    "        # Convert to dB scale\n",
    "        log_mel_spec = self.amplitude_to_db(mel_spec)\n",
    "        \n",
    "        # Resize to target dimensions\n",
    "        log_mel_spec = torch.tensor(\n",
    "            np.resize(log_mel_spec.numpy(), IMG_SIZE)\n",
    "        )\n",
    "        \n",
    "        # Normalize\n",
    "        log_mel_spec = (log_mel_spec - log_mel_spec.mean()) / (log_mel_spec.std() + 1e-10)\n",
    "        \n",
    "        return log_mel_spec.unsqueeze(0)  # Add channel dimension\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_idx, segment_idx = self.segment_indices[idx]\n",
    "        file_path, label = self.file_list[file_idx]\n",
    "        \n",
    "        # Load audio\n",
    "        waveform = self._load_audio(file_path)\n",
    "        \n",
    "        # Extract segment\n",
    "        start_sample = segment_idx * SEGMENT_SAMPLES\n",
    "        end_sample = start_sample + SEGMENT_SAMPLES\n",
    "        segment = waveform[start_sample:end_sample]\n",
    "        \n",
    "        # Extract features\n",
    "        features = self._extract_features(segment)\n",
    "        \n",
    "        # Apply transforms if available\n",
    "        if self.transform and self.mode == 'train':\n",
    "            features = self.transform(features)\n",
    "            \n",
    "        return features.float(), torch.tensor(label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80fec62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77af857a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    \"\"\"Hybrid CNN-Transformer with Multi-Head Self Attention\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=len(CLASSES), num_heads=4, embed_dim=256):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # Convolutional Feature Extractor\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            # Block 1\n",
    "            nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.GELU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            \n",
    "            # Block 2 with attention\n",
    "            ResidualBlock(64, 128, downsample=True),\n",
    "            SelfAttentionBlock(128, num_heads),\n",
    "            \n",
    "            # Block 3 with attention\n",
    "            ResidualBlock(128, embed_dim, downsample=True),\n",
    "            SelfAttentionBlock(embed_dim, num_heads),\n",
    "            \n",
    "            nn.Dropout(0.4)\n",
    "        )\n",
    "        \n",
    "        # Transformer Encoder\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=embed_dim,\n",
    "                nhead=num_heads,\n",
    "                dim_feedforward=embed_dim*4,\n",
    "                dropout=0.3,\n",
    "                activation='gelu',\n",
    "                batch_first=True\n",
    "            ),\n",
    "            num_layers=2\n",
    "        )\n",
    "        \n",
    "        # Adaptive pooling and classifier\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(embed_dim),\n",
    "            nn.Linear(embed_dim, embed_dim*2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(embed_dim*2, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolutional features\n",
    "        x = self.conv_layers(x)  # [B, 256, H', W']\n",
    "        \n",
    "        # Prepare for transformer\n",
    "        b, c, h, w = x.size()\n",
    "        x = x.view(b, c, h*w).permute(0, 2, 1)  # [B, seq_len, embed_dim]\n",
    "        \n",
    "        # Transformer processing\n",
    "        x = self.transformer(x)\n",
    "        \n",
    "        # Pooling and classification\n",
    "        x = x.mean(dim=1)  # Global average pooling\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4a0df7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"Residual block with depthwise separable convolutions\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, downsample=False):\n",
    "        super().__init__()\n",
    "        stride = 2 if downsample else 1\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels, 3, stride, 1, groups=in_channels),\n",
    "            nn.Conv2d(in_channels, out_channels, 1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if in_channels != out_channels or downsample:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, 1, stride),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x) + self.shortcut(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c96b230",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionBlock(nn.Module):\n",
    "    \"\"\"Multi-head self attention with positional encoding\"\"\"\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.attn = nn.MultiheadAttention(\n",
    "            embed_dim, \n",
    "            num_heads,\n",
    "            dropout=0.2,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim*4),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(embed_dim*4, embed_dim)\n",
    "        )\n",
    "        \n",
    "        # Learnable scale parameter for attention contribution\n",
    "        self.alpha = nn.Parameter(torch.tensor(0.1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.size()\n",
    "        x_flat = x.view(B, C, H*W).permute(0, 2, 1)\n",
    "        \n",
    "        # Self-attention with residual\n",
    "        attn_out, _ = self.attn(x_flat, x_flat, x_flat)\n",
    "        x = x + self.alpha * attn_out.permute(0, 2, 1).view_as(x)\n",
    "        \n",
    "        # Feedforward with residual\n",
    "        x_flat = x.view(B, C, H*W).permute(0, 2, 1)\n",
    "        x = x + self.ffn(self.norm(x_flat)).permute(0, 2, 1).view_as(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b25297b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def prepare_datasets(data_path):\n",
    "    # Collect all audio files with their labels\n",
    "    file_list = []\n",
    "    for class_name in CLASSES:\n",
    "        class_dir = os.path.join(data_path, class_name)\n",
    "        if os.path.isdir(class_dir):\n",
    "            for f in os.listdir(class_dir):\n",
    "                if f.endswith('.wav'):\n",
    "                    file_path = os.path.join(class_dir, f)\n",
    "                    file_list.append((file_path, CLASS_MAPPING[class_name]))\n",
    "    \n",
    "    # Create segment indices for stratified split\n",
    "    np.random.shuffle(file_list)\n",
    "    file_indices = list(range(len(file_list)))\n",
    "    \n",
    "    # Split files into train/val/test (70/15/15)\n",
    "    train_files = []\n",
    "    val_files = []\n",
    "    test_files = []\n",
    "    \n",
    "    for class_idx in range(len(CLASSES)):\n",
    "        class_files = [f for f in file_list if f[1] == class_idx]\n",
    "        n = len(class_files)\n",
    "        n_train = int(n * 0.95)\n",
    "        n_val = int(n * 0.02)\n",
    "        \n",
    "        train_files.extend(class_files[:n_train])\n",
    "        val_files.extend(class_files[n_train:n_train+n_val])\n",
    "        test_files.extend(class_files[n_train+n_val:])\n",
    "    \n",
    "    # Generate segment indices for each split\n",
    "    def generate_segment_indices(files):\n",
    "        indices = []\n",
    "        for file_idx, (file_path, label) in enumerate(files):\n",
    "            for seg_idx in range(NUM_SEGMENTS):\n",
    "                indices.append((file_idx, seg_idx))\n",
    "        return indices\n",
    "    \n",
    "    train_segment_indices = generate_segment_indices(train_files)\n",
    "    val_segment_indices = generate_segment_indices(val_files)\n",
    "    test_segment_indices = generate_segment_indices(test_files)\n",
    "    \n",
    "    # Create datasets with mixup augmentation\n",
    "    train_dataset = OnTheFlyMusicGenreDataset(\n",
    "        train_files,\n",
    "        train_segment_indices,\n",
    "        transform=None,  # Add other transforms if needed\n",
    "        mode='train'\n",
    "    )\n",
    "    \n",
    "    val_dataset = OnTheFlyMusicGenreDataset(\n",
    "        val_files,\n",
    "        val_segment_indices,\n",
    "        mode='val'\n",
    "    )\n",
    "    \n",
    "    test_dataset = OnTheFlyMusicGenreDataset(\n",
    "        test_files,\n",
    "        test_segment_indices,\n",
    "        mode='test'\n",
    "    )\n",
    "    \n",
    "    return train_dataset, val_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb181a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(batch):\n",
    "    \"\"\"Custom collate function to apply mixup\"\"\"\n",
    "    mixup = Mixup(alpha=0.4)\n",
    "    return mixup(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0dbdea",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def save_train_history(train_losses, val_losses, train_accs, val_accs, filename='training_history.png'):\n",
    "    \"\"\"Save training history as a plot\"\"\"\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Plot training & validation accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_accs)\n",
    "    plt.plot(val_accs)\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    \n",
    "    # Plot training & validation loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_losses)\n",
    "    plt.plot(val_losses)\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b59a88e",
   "metadata": {},
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs, device):\n",
    "    \"\"\"Train the model and return training history\"\"\"\n",
    "    best_val_acc = 0.0\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accs = []\n",
    "    val_accs = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcdb9ed",
   "metadata": {},
   "source": [
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c0cc8a",
   "metadata": {},
   "source": [
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16aa9f43",
   "metadata": {},
   "source": [
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5addd11f",
   "metadata": {},
   "source": [
    "            # Forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d0c873",
   "metadata": {},
   "source": [
    "            # Statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee3d4b0",
   "metadata": {},
   "source": [
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_acc = correct / total\n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accs.append(epoch_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4966e4d",
   "metadata": {},
   "source": [
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9005db8d",
   "metadata": {},
   "source": [
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7aec4a",
   "metadata": {},
   "source": [
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269684a3",
   "metadata": {},
   "source": [
    "                # Statistics\n",
    "                val_running_loss += loss.item() * inputs.size(0)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6365cd13",
   "metadata": {},
   "source": [
    "        val_epoch_loss = val_running_loss / len(val_loader.dataset)\n",
    "        val_epoch_acc = val_correct / val_total\n",
    "        val_losses.append(val_epoch_loss)\n",
    "        val_accs.append(val_epoch_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1242998",
   "metadata": {},
   "source": [
    "        # Update learning rate\n",
    "        scheduler.step(val_epoch_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276751e5",
   "metadata": {},
   "source": [
    "        # Print statistics\n",
    "        print(f'Epoch {epoch+1}/{epochs}, '\n",
    "              f'Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.4f}, '\n",
    "              f'Val Loss: {val_epoch_loss:.4f}, Val Acc: {val_epoch_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b29f1a",
   "metadata": {},
   "source": [
    "        # Save best model\n",
    "        if val_epoch_acc > best_val_acc:\n",
    "            best_val_acc = val_epoch_acc\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            print(f\"Saved new best model with validation accuracy: {best_val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe745d50",
   "metadata": {},
   "source": [
    "    # Load best model\n",
    "    model.load_state_dict(torch.load('best_model.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd0e317",
   "metadata": {},
   "source": [
    "    return model, train_losses, val_losses, train_accs, val_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d8337c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# def evaluate_model(model, test_loader, device):\n",
    "    \"\"\"Evaluate the model on test data\"\"\"\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = np.mean(np.array(all_predictions) == np.array(all_labels))\n",
    "    \n",
    "    # Generate classification report\n",
    "    report = classification_report(\n",
    "        all_labels, \n",
    "        all_predictions, \n",
    "        target_names=CLASSES, \n",
    "        output_dict=True\n",
    "    )\n",
    "    \n",
    "    # Print classification report\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(all_labels, all_predictions, target_names=CLASSES))\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_predictions)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=CLASSES, yticklabels=CLASSES)\n",
    "    plt.xlabel(\"Predicted Labels\")\n",
    "    plt.ylabel(\"True Labels\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"confusion_matrix.png\")\n",
    "    \n",
    "    return accuracy, report, cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55650dce",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# class Mixup:\n",
    "    \"\"\"Mixup augmentation implementation\"\"\"\n",
    "    def __init__(self, alpha=0.4):\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        inputs, labels = zip(*batch)\n",
    "        inputs = torch.stack(inputs)\n",
    "        labels = torch.stack(labels)\n",
    "        \n",
    "        # Generate mixup coefficients\n",
    "        lam = np.random.beta(self.alpha, self.alpha)\n",
    "        batch_size = inputs.size(0)\n",
    "        index = torch.randperm(batch_size)\n",
    "        \n",
    "        # Mix inputs\n",
    "        mixed_inputs = lam * inputs + (1 - lam) * inputs[index]\n",
    "        \n",
    "        # Mix labels (convert to one-hot)\n",
    "        labels_onehot = F.one_hot(labels, num_classes=len(CLASSES)).float()\n",
    "        mixed_labels = lam * labels_onehot + (1 - lam) * labels_onehot[index]\n",
    "        \n",
    "        return mixed_inputs, mixed_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb9cd2c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs, device):\n",
    "    best_val_loss = float('inf')\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_correct = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Handle mixup labels\n",
    "            is_mixup = labels.dim() > 1\n",
    "            if is_mixup:\n",
    "                # For mixup, use KLDivLoss with log probabilities\n",
    "                outputs = model(inputs)\n",
    "                log_probs = F.log_softmax(outputs, dim=1)\n",
    "                loss = F.kl_div(log_probs, labels, reduction='batchmean')\n",
    "                \n",
    "                # For accuracy calculation, use dominant class\n",
    "                class_labels = labels.argmax(dim=1)\n",
    "            else:\n",
    "                # Normal training\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                class_labels = labels\n",
    "                \n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Calculate metrics\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_correct += (predicted == class_labels).sum().item()\n",
    "            total_samples += inputs.size(0)\n",
    "            \n",
    "            # Progress update\n",
    "            if (batch_idx + 1) % 50 == 0:\n",
    "                current_loss = running_loss / total_samples\n",
    "                current_acc = running_correct / total_samples\n",
    "                print(f'Epoch [{epoch+1}/{epochs}], Batch [{batch_idx+1}/{len(train_loader)}], '\n",
    "                      f'Loss: {current_loss:.4f}, Acc: {current_acc:.4f}')\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        val_running_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_running_loss += loss.item() * inputs.size(0)\n",
    "                val_running_correct += (predicted == labels).sum().item()\n",
    "                val_total += inputs.size(0)\n",
    "        \n",
    "        # Calculate epoch metrics\n",
    "        train_loss = running_loss / total_samples\n",
    "        train_acc = running_correct / total_samples\n",
    "        val_loss = val_running_loss / val_total\n",
    "        val_acc = val_running_correct / val_total\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            print(f'New best model saved with val_loss: {val_loss:.4f}')\n",
    "        \n",
    "        # Store history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs}: '\n",
    "              f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | '\n",
    "              f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "    \n",
    "    return model, history['train_loss'], history['val_loss'], history['train_acc'], history['val_acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcb433c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    report = classification_report(all_labels, all_preds, target_names=CLASSES, zero_division=0)\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    \n",
    "    return accuracy, report, cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c3a4fc",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class Mixup:\n",
    "    \"\"\"Improved Mixup implementation with proper label handling\"\"\"\n",
    "    def __init__(self, alpha=0.4):\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        inputs, labels = zip(*batch)\n",
    "        inputs = torch.stack(inputs)\n",
    "        labels = torch.stack(labels)\n",
    "        \n",
    "        # Generate mixup coefficients\n",
    "        lam = np.random.beta(self.alpha, self.alpha)\n",
    "        batch_size = inputs.size(0)\n",
    "        index = torch.randperm(batch_size)\n",
    "        \n",
    "        # Mix inputs\n",
    "        mixed_inputs = lam * inputs + (1 - lam) * inputs[index]\n",
    "        \n",
    "        # Convert labels to one-hot\n",
    "        labels_onehot = F.one_hot(labels, num_classes=len(CLASSES)).float()\n",
    "        \n",
    "        # Mix labels\n",
    "        mixed_labels = lam * labels_onehot + (1 - lam) * labels_onehot[index]\n",
    "        \n",
    "        return mixed_inputs, mixed_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e2762a",
   "metadata": {},
   "source": [
    "def main():\n",
    "    print(\"Starting music genre classification with PyTorch...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7ffd61",
   "metadata": {},
   "source": [
    "    # Collect file paths and labels\n",
    "    file_paths = []\n",
    "    labels = []\n",
    "    segment_indices = []\n",
    "    metadata = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18677c2",
   "metadata": {},
   "source": [
    "    print(\"Collecting audio file paths...\")\n",
    "    for class_name in CLASSES:\n",
    "        class_dir = os.path.join(DATA_PATH, class_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e02848",
   "metadata": {},
   "source": [
    "        if not os.path.isdir(class_dir):\n",
    "            print(f\"Warning: Class directory '{class_dir}' not found\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98ad1c7",
   "metadata": {},
   "source": [
    "        print(f\"Processing class: {class_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adbb207",
   "metadata": {},
   "source": [
    "        # Get all WAV files in the class directory\n",
    "        wav_files = [f for f in os.listdir(class_dir) if f.endswith('.wav')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b2b044",
   "metadata": {},
   "source": [
    "        if len(wav_files) == 0:\n",
    "            print(f\"Warning: No WAV files found in {class_dir}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c927c9",
   "metadata": {},
   "source": [
    "        print(f\"Found {len(wav_files)} files in {class_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b27a5c",
   "metadata": {},
   "source": [
    "        # Collect file paths and labels\n",
    "        for f in wav_files:\n",
    "            file_path = os.path.join(class_dir, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839bf065",
   "metadata": {},
   "source": [
    "            # For each audio file, we will create NUM_SEGMENTS data points\n",
    "            for segment_idx in range(NUM_SEGMENTS):\n",
    "                file_paths.append(file_path)\n",
    "                labels.append(CLASS_MAPPING[class_name])\n",
    "                segment_indices.append(segment_idx)\n",
    "                metadata.append({\n",
    "                    'file': f,\n",
    "                    'class': class_name,\n",
    "                    'segment': segment_idx\n",
    "                })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0a8397",
   "metadata": {},
   "source": [
    "    print(f\"Total samples: {len(file_paths)} ({len(file_paths) // NUM_SEGMENTS} files Ã— {NUM_SEGMENTS} segments)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fecfb11",
   "metadata": {},
   "source": [
    "    # Create custom sampler to ensure all segments from the same file go to the same split\n",
    "    # First, group indices by file\n",
    "    file_groups = {}\n",
    "    for i, meta in enumerate(metadata):\n",
    "        file_key = f\"{meta['class']}_{meta['file']}\"\n",
    "        if file_key not in file_groups:\n",
    "            file_groups[file_key] = []\n",
    "        file_groups[file_key].append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd11334",
   "metadata": {},
   "source": [
    "    # Group file indices by class\n",
    "    files_by_class = {}\n",
    "    for file_key, indices in file_groups.items():\n",
    "        class_name = file_key.split('_')[0]\n",
    "        if class_name not in files_by_class:\n",
    "            files_by_class[class_name] = []\n",
    "        files_by_class[class_name].append(indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74570182",
   "metadata": {},
   "source": [
    "    # Split files into train, validation, and test sets while maintaining class balance\n",
    "    train_indices = []\n",
    "    val_indices = []\n",
    "    test_indices = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccc8ad4",
   "metadata": {},
   "source": [
    "    for class_name, file_indices_list in files_by_class.items():\n",
    "        # Shuffle the files for this class\n",
    "        np.random.shuffle(file_indices_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a238d92",
   "metadata": {},
   "source": [
    "        # Split: 70% train, 15% validation, 15% test\n",
    "        n_files = len(file_indices_list)\n",
    "        n_train = int(n_files * 0.7)\n",
    "        n_val = int(n_files * 0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29e62f5",
   "metadata": {},
   "source": [
    "        # Add all segments from each file to the appropriate set\n",
    "        for i, file_indices in enumerate(file_indices_list):\n",
    "            if i < n_train:\n",
    "                train_indices.extend(file_indices)\n",
    "            elif i < n_train + n_val:\n",
    "                val_indices.extend(file_indices)\n",
    "            else:\n",
    "                test_indices.extend(file_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cb54bf",
   "metadata": {},
   "source": [
    "    # Create datasets for each split\n",
    "    train_dataset = MusicGenreDataset(\n",
    "        [file_paths[i] for i in train_indices],\n",
    "        [labels[i] for i in train_indices],\n",
    "        [segment_indices[i] for i in train_indices]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273bbfc2",
   "metadata": {},
   "source": [
    "    val_dataset = MusicGenreDataset(\n",
    "        [file_paths[i] for i in val_indices],\n",
    "        [labels[i] for i in val_indices],\n",
    "        [segment_indices[i] for i in val_indices]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fa274b",
   "metadata": {},
   "source": [
    "    test_dataset = MusicGenreDataset(\n",
    "        [file_paths[i] for i in test_indices],\n",
    "        [labels[i] for i in test_indices],\n",
    "        [segment_indices[i] for i in test_indices]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486eec0c",
   "metadata": {},
   "source": [
    "    print(f\"Training set: {len(train_dataset)} samples\")\n",
    "    print(f\"Validation set: {len(val_dataset)} samples\")\n",
    "    print(f\"Test set: {len(test_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a218c49",
   "metadata": {},
   "source": [
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=True, \n",
    "        num_workers=4, \n",
    "        pin_memory=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489a8a9f",
   "metadata": {},
   "source": [
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=False, \n",
    "        num_workers=4, \n",
    "        pin_memory=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f82a186",
   "metadata": {},
   "source": [
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=False, \n",
    "        num_workers=4, \n",
    "        pin_memory=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23a5a07",
   "metadata": {},
   "source": [
    "    # Build the model\n",
    "    # print(\"Building CNN model...\")\n",
    "    model = CNNModel(num_classes=len(CLASSES))\n",
    "    model = model.to(device)\n",
    "    # print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f85fd6",
   "metadata": {},
   "source": [
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e30c80b",
   "metadata": {},
   "source": [
    "    # Learning rate scheduler\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, \n",
    "        mode='min', \n",
    "        factor=0.5, \n",
    "        patience=5, \n",
    "        min_lr=1e-6, \n",
    "        verbose=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db499453",
   "metadata": {},
   "source": [
    "    # Train the model\n",
    "    print(\"Training the model...\")\n",
    "    model, train_losses, val_losses, train_accs, val_accs = train_model(\n",
    "        model, \n",
    "        train_loader, \n",
    "        val_loader, \n",
    "        criterion, \n",
    "        optimizer, \n",
    "        scheduler, \n",
    "        EPOCHS, \n",
    "        device\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb22a70f",
   "metadata": {},
   "source": [
    "    # Save the training history\n",
    "    save_train_history(train_losses, val_losses, train_accs, val_accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56046514",
   "metadata": {},
   "source": [
    "    # Save the final model\n",
    "    torch.save(model.state_dict(), \"cnn_model.pth\")\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'class_names': CLASSES,\n",
    "        'img_size': IMG_SIZE,\n",
    "    }, \"cnn_model_full.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02efdd60",
   "metadata": {},
   "source": [
    "    # Evaluate on test set\n",
    "    print(\"Evaluating on test set...\")\n",
    "    test_acc, report, cm = evaluate_model(model, test_loader, device)\n",
    "    print(f\"Test accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09979927",
   "metadata": {},
   "source": [
    "    # Save model architecture as text file\n",
    "    with open('model_summary.txt', 'w') as f:\n",
    "        f.write(str(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07df34d0",
   "metadata": {},
   "source": [
    "    print(\"Model training and evaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2829e0c",
   "metadata": {},
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98da863b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Prepare datasets\n",
    "    train_dataset, val_dataset, test_dataset = prepare_datasets(DATA_PATH)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "        collate_fn=custom_collate\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Model setup (same as original)\n",
    "    model = CNNModel().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, \n",
    "        mode='min', \n",
    "        factor=0.5, \n",
    "        patience=5, \n",
    "        min_lr=1e-6, \n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    # Training loop (same as original)\n",
    "    model, train_losses, val_losses, train_accs, val_accs = train_model(\n",
    "        model, train_loader, val_loader, criterion, optimizer, scheduler, EPOCHS, device\n",
    "    )\n",
    "    \n",
    "    # Evaluation (same as original)\n",
    "    test_acc, report, cm = evaluate_model(model, test_loader, device)\n",
    "    print(f\"Final Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00070d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "vit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
